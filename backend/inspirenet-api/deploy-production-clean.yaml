apiVersion: serving.knative.dev/v1
kind: Service
metadata:
  name: inspirenet-bg-removal-api
  annotations:
    run.googleapis.com/ingress: all
    run.googleapis.com/description: "Production InSPyReNet API - Cost Optimized with smart warming strategies"
spec:
  template:
    metadata:
      annotations:
        # Execution environment
        run.googleapis.com/execution-environment: gen2
        
        # Scaling configuration - COST OPTIMIZED: minScale 0 with smart warming
        # COST BENEFIT: $0 base cost vs $65-100/day for constant GPU instance
        # WARMING STRATEGY: Combines multiple cost-effective approaches:
        # 
        # Cold start challenge:
        # - Model loading: 20-30s (PyTorch + InSPyReNet model)
        # - Container startup: 10-15s (GPU allocation + dependencies)
        # - Total delay: 30-60s (only on first request after idle)
        #
        # Cost-effective solutions implemented:
        # 1. Frontend prewarming: Warm API when user shows intent (hover, page load)
        # 2. Business hours warmup: Cloud Scheduler keeps warm 9AM-9PM
        # 3. Optimized cold starts: Faster model loading and startup
        # 4. Progressive loading: Show accurate progress during cold starts
        #
        # Result: Near-zero cost with acceptable user experience
        autoscaling.knative.dev/minScale: "0"
        autoscaling.knative.dev/maxScale: "3"
        autoscaling.knative.dev/maxConcurrency: "1"
        
        # Performance optimizations
        run.googleapis.com/startup-cpu-boost: "true"
        run.googleapis.com/cpu-throttling: "false"
        
        # Health check configuration
        run.googleapis.com/health-check-path: "/health"
        run.googleapis.com/health-check-interval: "30s"
        run.googleapis.com/health-check-timeout: "10s"
        run.googleapis.com/health-check-failure-threshold: "3"
        
        # Startup probe configuration - Extended timeout for GPU model loading
        run.googleapis.com/startup-probe-path: "/health"
        run.googleapis.com/startup-probe-initial-delay: "60s"
        run.googleapis.com/startup-probe-timeout: "30s"
        run.googleapis.com/startup-probe-period: "30s"
        run.googleapis.com/startup-probe-failure-threshold: "10"
        
        # Timeout configuration - Extended for GPU model loading
        run.googleapis.com/timeout: "600s"
        
        # Memory configuration
        run.googleapis.com/memory: "32Gi"
        
        # GPU configuration
        run.googleapis.com/gpu-zonal-redundancy-disabled: "true"
        
    spec:
      containers:
      - image: us-central1-docker.pkg.dev/perkieprints-processing/pet-bg-removal/inspirenet-bg-removal-api:critical-fix
        ports:
        - containerPort: 8080
          name: http1
        resources:
          limits:
            cpu: '8'
            memory: '32Gi'
            nvidia.com/gpu: '1'
          requests:
            cpu: '4'
            memory: '32Gi'
            nvidia.com/gpu: '1'
        env:
        # Environment identification
        - name: DEPLOYMENT_ENV
          value: "production"
        
        # Model configuration
        - name: MODEL_PATH
          value: "/app/models/inspirenet.pth"
        - name: TARGET_SIZE
          value: "1024"
        - name: INSPIRENET_MODE
          value: "base"
        - name: INSPIRENET_RESIZE
          value: "dynamic"
        - name: MODEL_LOAD_TIMEOUT
          value: "120"
        
        # Storage configuration
        - name: STORAGE_BUCKET
          value: "perkieprints-processing-cache"
        - name: CUSTOMER_STORAGE_BUCKET
          value: "perkieprints-customer-images"
        - name: GCS_BUCKET_NAME
          value: "perkieprints-processing-cache"
        - name: CACHE_TTL
          value: "86400"
        
        # Performance configuration - COST OPTIMIZED: MIN_INSTANCES = 0 with smart warming
        # This environment variable should match autoscaling.knative.dev/minScale above
        - name: MIN_INSTANCES
          value: "0"
        - name: MAX_CONCURRENT_REQUESTS
          value: "1"
        - name: MAX_PARALLEL_EFFECTS
          value: "3"
        
        # Feature toggles (production settings)
        - name: ENABLE_OPTIMIZATIONS
          value: "true"
        - name: ENABLE_WARMUP
          value: "false"  # Disabled since MIN_INSTANCES = 0, use external warming strategies
        - name: ENABLE_MEMORY_MONITORING
          value: "true"
        - name: ENABLE_GPU_OPTIMIZATIONS
          value: "true"
        
        # Memory thresholds (production settings)
        - name: MEMORY_THRESHOLD_CPU
          value: "0.75"
        - name: MEMORY_THRESHOLD_GPU
          value: "0.80"
        - name: MEMORY_CLEANUP_INTERVAL
          value: "30"
        
        # System configuration
        - name: GOOGLE_CLOUD_PROJECT
          value: "perkieprints-processing"
        - name: LOG_LEVEL
          value: "info"
        
        # Resource limits
        - name: MAX_IMAGE_SIZE_MB
          value: "30"
        - name: MOBILE_MAX_SIZE
          value: "1280"
        - name: EFFECTS_BATCH_SIZE
          value: "2"
        
        # PyTorch and CUDA configuration (optimized for cold start performance)
        - name: OMP_NUM_THREADS
          value: "2"  # Reduced from 4 to speed up initial loading
        - name: MKL_NUM_THREADS
          value: "2"  # Reduced for faster startup
        - name: NUMEXPR_NUM_THREADS
          value: "2"  # Reduced for faster startup
        - name: CUDA_VISIBLE_DEVICES
          value: "0"
        - name: NVIDIA_VISIBLE_DEVICES
          value: "0"
        - name: PYTORCH_CUDA_ALLOC_CONF
          value: "max_split_size_mb:64,expandable_segments:True,garbage_collection_threshold:0.8"
        
        # Additional cold start optimizations
        - name: PYTHONOPTIMIZE
          value: "2"  # Enable optimizations, remove docstrings and debug info
        - name: PYTHONDONTWRITEBYTECODE
          value: "1"  # Don't write .pyc files (faster startup)
        - name: TORCH_CACHE_DIR
          value: "/app/models/torch_cache"  # Consolidated cache location
        - name: TRANSFORMERS_CACHE
          value: "/app/models/transformers_cache"  # Faster model loading
        - name: CUDA_LAUNCH_BLOCKING
          value: "0"  # Non-blocking CUDA operations for better performance
        
        # Cold start optimization - Model loading strategy
        - name: MODEL_PRELOAD_STRATEGY
          value: "lazy"  # Load model components on-demand to reduce startup time
        - name: TORCH_JIT_COMPILATION
          value: "false"  # Disable JIT compilation during cold start to speed up initial load
        - name: CUDNN_BENCHMARK
          value: "false"  # Disable cuDNN benchmarking during cold start
        - name: WARMUP_REQUESTS_ENABLED
          value: "true"  # Enable warmup request handling for scheduler
        
        # Memory optimization for faster startup
        - name: TORCH_SHARING_STRATEGY
          value: "file_descriptor"  # Use file descriptors for faster memory sharing
        - name: MALLOC_MMAP_THRESHOLD_
          value: "32768"  # Use mmap for allocations > 32KB (faster startup)
        
        # Progressive loading optimization
        - name: ENABLE_PROGRESSIVE_LOADING
          value: "true"  # Load model weights progressively during first request
        - name: BACKGROUND_MODEL_OPTIMIZATION
          value: "true"  # Optimize model in background after first load